'''
This script is the processing pipeline used to create the "automatic" datasets files shown in DGML.
 The automatic datasets are those that are not curated but created by this script.
 There are ___ steps in this pipeline:

   1. Get csv files' paths from dataset_path input parameter
   2. Filter csv paths according to a specific_ids text file
   3. Load DGF catalog to retrieve info about datasets
   4. For each dataset (csv) path, do:
        a. Return the csv dataframe and its metadata load_wrapper()
        b. Create an output folder with the id of the dataset as name
        c. Discard datasets not passing the first level constraints
        d. Generate pandas profiling. Create file {id}_pandas_profile.html
        e. Generate pandas profiling summary. Create file statistics_summary.csv
        f. Get data dictionary. Create file dict_data.csv
        g. Returns clean dataset before mljar.
        h. Discard empty datasets and datasets with less than 3 columns. Save
           its pandas profile report.
        i. For each target variable (column) in the dataset, do:
            1. Remove missing values in target variable column
            2. Run mljar process. Save output files in {dataset_id}/{automl_target-var}/
            3. Compute and store a score to this mljar run with this dataset and with this target var
            4. Add a new line to the open_data_ml_datasets.csv file with the info of this dataset
               (pandas profile + mljar profile for this run, etc)



Notes:
    1. We remove non-ascii characters within the categorical (nominal) values (it does not work if we don't do this)
    2. We remove the lines with missing values in the target variable

'''

import glob
import logging
from pathlib import Path
from typing import Union, Optional

import fire
from pandas.util import hash_pandas_object
from dotenv import load_dotenv
import fs
import pandas as pd
from csv_detective.explore_csv import routine
from fs.glob import GlobMatch
from supervised.model_framework import ModelFramework
from src.get_dataset import latest_catalog, info_from_catalog, load_dataset
from src.get_mljar import prepare_for_mljar, generate_mljar
from src.get_statistic_summary import generate_pandas_profiling, get_statistics_summary, get_data_dictionary
from app.apps.utils import slugify
import json
import zipfile

load_dotenv(".env")
logging.root.handlers = []
# noinspection PyArgumentList
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("debug.log", mode="w"),
        logging.StreamHandler()
    ]
)


def get_specific_ids(specific_ids_path: Optional[Path] = None):
    """
    If there is a specific_ids_path file, take the ids within and process them
    exclusively.
    :param specific_ids_path:  The path of a text file with id per line
    :return: List of read ids from the specific_ids_path file
    """
    if specific_ids_path is None or not specific_ids_path.exists():
        return
    with open(specific_ids_path) as filo:
        specific_ids = [l.strip() for l in filo.readlines()]
    logging.info(f"We found specific ids. They are: {specific_ids}")
    return specific_ids


def create_folder(output_dir):
    if not output_dir.exists():
        output_dir.mkdir()


def get_mljar_info(output_dir, automl_report):
    """

    :param output_dir:
    :param automl_report:
    :return:
    """
    # 1. Move the leaderboard.csv file (automl summary) to the upper level
    automl_report.get_leaderboard().to_csv(output_dir.joinpath("leaderboard.csv"), index=False)

    # 2. Delete all models (bc they are heavy and we don't use them)
    model_file_paths = [Path(p) for p in glob.glob(output_dir.as_posix() + f"/**/learner_fold_*.*", recursive=True)]
    model_file_paths = [p for p in model_file_paths if
                        p.suffix not in [".csv", ".png", ".log", ".svg"]]
    for model_path in model_file_paths:
        model_path.unlink()


def fill_main_csv(id_, catalog, statistics_summary, output_dir=Path("../app/assets/datasets/"),
                  target_variable=None, task=None, score='', automl=None):
    """This function adds a new row in dgml_datasets.csv containing info of a chosen dataset."""
    main_csv_path = output_dir.joinpath('dgml_datasets.csv')
    new_row = {}
    dict_main_df = {'title': 'dataset.title', 'dgf_dataset_url': 'dataset.url',
                    'dgf_dataset_id': 'dataset.id', 'dgf_resource_url': 'url','description':'description'}
    for key, item in dict_main_df.items():
        new_row[key] = catalog[catalog['id'] == id_][item].values.item()
    new_row['nb_lines'] = statistics_summary['Number of lines'][0]
    new_row['nb_features'] = statistics_summary['Number of variables'][0]
    new_row['profile_url'] = f""
    if target_variable is None and task is None:
        new_row['automl_url'] = ''
        new_row['target_variable'] = ''
        new_row['task'] = ''
    elif task is 'EDA':
        new_row['automl_url'] = ''
        new_row['target_variable'] = ''
        new_row['task'] = 'EDA'
    else:
        new_row['automl_url'] = ''
        new_row['target_variable'] = target_variable
        if "classification" in task.lower():
            new_row["task"] = "Classification"
        else:
            new_row["task"] = task.capitalize()
    new_row["is_validated"] = False
    new_row["topic"] = "Undefined"
    new_row["dict_url"] = ""
    new_row['dgf_resource_id'] = id_
    new_row['openml_id'] = ""
    # add info about best model:
    if automl is None:
        new_row['best_model'] = ''
        new_row['metric_type'] = ''
        new_row['metric_value'] = ''
    else:
        if isinstance(automl._best_model, ModelFramework):
            new_row['best_model'] = automl._best_model.get_name()
            new_row['metric_type'] = automl._best_model.metric_name
            new_row['metric_value'] = automl._best_model.get_final_loss()
        else:
            new_row['best_model'] = automl._best_model.algorithm_short_name
            new_row['metric_type'] = automl._get_eval_metric()
            new_row['metric_value'] = automl._best_model.best_loss
    if main_csv_path.exists():
        main_df = pd.read_csv(main_csv_path)
        main_df = main_df.append(new_row, ignore_index=True)
    else:
        main_df = pd.DataFrame([new_row])
    # add score to main csv file:
    main_df.loc[
        (main_df['dgf_resource_id'] == id_) & (main_df['target_variable'] == target_variable), ['score']] = score
    main_df.to_csv(main_csv_path, index=False)
    return main_df


def check_constraints(data, parameters):
    """This function filters the datasets in the datasets folder according to the basic constraints defined in the config file
    (config/filters.json).
    In fact, only datasets that have the following characteristics will be included in DGML's analysis:
    * minimal  NUMBER OF LINES (param: min_lines)
    * maximal number of lines (param: max_lines)
    * minimal NUMBER OF COLUMNS (param: min_cols)
    * maximal number of columns (param: max_cols)
    * minimal LINES/COLUMNS RATIO ; ex. if 10: the dataset contains at least 10 times more
    lines than columns (param: lines_cols/ratio)
    * the presence of BOTH NUMERICAL AND CATEGORICAL VARIABLES ; if TRUE: the dataset must contain
    both categorical and numerical variables (param: num_and_cat)
    * the maximum percentage of MISSING VALUES (param:max_missing_values)
    """
    passed_constraints = False
    nb_lines = len(data)
    nb_columns = len(data.columns)
    check_categorical = data.select_dtypes(include='object').empty
    check_numerical = data.select_dtypes(include=['float64', 'int64']).empty
    for param in parameters:
        min_lines = float(param['min_lines'])
        max_lines = float(param['max_lines'])
        min_cols = float(param['min_cols'])
        max_cols = float(param['max_cols'])
        lines_cols_ratio = float(param['lines_cols/ratio'])
        max_missing = float(param['max_missing_values'])
    total_nan = data.isna().sum().sum() / (nb_lines * nb_columns)
    if (min_lines <= nb_lines <= max_lines) and (min_cols <= nb_columns <= max_cols) and (
            (nb_lines / nb_columns) >= lines_cols_ratio) and (
            check_categorical is False) and (check_numerical is False) and (total_nan <= max_missing):
        passed_constraints = True
    return passed_constraints


def load_dataset_wrapper(dataset_name: Union[Path, str]):
    csv_data = None
    if isinstance(dataset_name, Path):
        try:
            csv_data = routine(dataset_name.as_posix(), num_rows=200)
        except Exception as e:
            logging.exception(f"Dataset {dataset_name}: csv-detective analysis failed")
            raise e

        encoding = csv_data.get("encoding", "latin-1")
        separator = csv_data.get("separator", ",")
        dataset_df = pd.read_csv(dataset_name, sep=separator, encoding=encoding)
        dataset_df.rename(columns=slugify, inplace=True)
        id_data = dataset_name.stem
        # Delete file if it is a temp file (in /tmp)
        if "/tmp" in dataset_name.as_posix():
            dataset_name.unlink()

    else:
        catalog = latest_catalog()  # or fixed_catalog to use our catalog
        catalog_info = info_from_catalog(dataset_name, catalog)
        dataset_df = load_dataset(id=dataset_name, catalog_info=catalog_info)
        id_data = dataset_name

    return dataset_df, id_data, csv_data


def get_csv_paths(datasets_path: Path):
    def create_files_iterator(remote_globber: GlobMatch):
        """

        :param remote_globber:
        :return:
        """
        temp_path = Path("/tmp").joinpath(Path(remote_globber.path).stem.split("--")[1] + ".csv")
        with open(temp_path, "wb") as tmp:
            my_fs.download(remote_globber.path, tmp)
        return Path(tmp.name)

    """Return the paths of each dataset in the source CSVs folder, whether local or through a sftp connection"""
    # 1. If the dataset_path is local, just return the list of csv files
    if "sftp" not in datasets_path.as_posix():
        if not datasets_path.exists():
            raise FileNotFoundError(f"File {datasets_path} not found. Please choose another csv folder")
        csv_paths = [Path(p) for p in glob.glob(datasets_path.as_posix() + f"/*.csv", recursive=True)]
    else:
        # 2. Try to connect to this remote resource (only dealing with sftp)
        # noinspection PyBroadException
        try:
            my_fs = fs.open_fs(datasets_path)
            csv_paths = (create_files_iterator(path) for path in my_fs.glob("**/*.csv").__iter__())
        except Exception as e:
            logging.exception(f"Connecting to {datasets_path} did not work")
            raise e
    return csv_paths


def generate_score(statistics_summary, columns_to_drop, automl):
    """Returns a score of the evaluating the 'goodness' of a given dataset. Datasets with an higher score will be selected for the app.
    The score takes into account:
    * the overall percentage of missing values, extracted from the pandas df of statistics_summary.csv (30%)
    * the percentage of variables not retained when running mljar (warning columns and columns detected by csv detective) (40%)
    * the logloss (for classification) or rmse (for regression) value for the best model (30%), extracted leaderboard.csv """
    prop_missing = statistics_summary['Percentage of missing cells'] / 100
    prop_not_retained = len(columns_to_drop) / statistics_summary['Number of variables']
    best_metric = automl.get_leaderboard()['metric_value'].min()
    score = 1 / (0.3 * prop_missing + 0.4 * prop_not_retained + 0.3 * best_metric)
    return score


def read_parameters(parameters_file: Path):
    """This function reads the config file containing the parameters needed for filtering datasets."""
    if parameters_file.exists():
        with open(parameters_file) as fout:
            parameters = json.load(fout)
    else:
        raise FileNotFoundError(f"Config file {parameters_file.as_posix()} does not exist.")
    return parameters


def extract_compressed_csv(dataset_path):
    """This function checks if a given dataset_path contains compressed csv files and extracts them if needed."""
    list_compressed = [Path(p) for p in glob.glob(dataset_path.joinpath("./*.zip").as_posix())]
    if len(list_compressed)!=0:
        for compressed_csv in list_compressed:
            with zipfile.ZipFile(compressed_csv, 'r') as zip_ref:
                zip_ref.extractall(dataset_path)


def main(dataset_path: str,
         output_dir: str,
         specific_ids_path: str = None,
         automl_mode: str = "Explain"):
    """

    :param dataset_path: Folder with CSVs files
    :param output_dir: Folder were the output is saved
    :param specific_ids_path: Path to a text file with a file name per line. Only this files will be treated
    :param automl_mode: "perform" ou "explain". Perform maximizes the model performance. Explain maximizes the
                        explicability of the models.
    :return: None
    """

    seen_dataframes = set()
    output_dir = Path(output_dir)
    dataset_path = Path(dataset_path)
    extract_compressed_csv(dataset_path)
    dataset_paths = get_csv_paths(dataset_path)
    specific_ids = get_specific_ids(specific_ids_path)
    automl_mode = automl_mode
    catalog = latest_catalog()  # or fixed_catalog to use our catalog
    for ix, path in enumerate(dataset_paths):
        if specific_ids and dataset_path.stem not in specific_ids:
            logging.warning(f"We are only analysing specific ids. Id {path.stem} is not a specified id. Trying "
                            f"next...")
            continue
        try:
            data_df, id_data, csv_detective_data = load_dataset_wrapper(path)
            df_hash = hash_pandas_object(data_df).sum()
            if df_hash in seen_dataframes:
                logging.info(f"Dataset {id_data}: Same hash dataset already treated. Skipping dataset.")
                continue
            seen_dataframes.add(df_hash)
            logging.info(f"Treating Dataset {id_data} ({ix})")
            current_output_dir = output_dir.joinpath(id_data)
            logging.info(f"Dataset {id_data}: Successfully loaded dataset.")
            create_folder(current_output_dir)
            create_folder(current_output_dir / "our_experiments")
            parameters = read_parameters(Path('src/config/filters.json'))
            if not check_constraints(data_df, parameters=parameters):
                logging.warning(
                    f"The Dataset {id_data} did not pass the first-level constraints. It seems not adequate for Machine "
                    f"Learning")
                continue
            logging.info(f"Dataset {id_data}: passed the first-level constraints")
            profiling = generate_pandas_profiling(id_data, data_df, output_dir=current_output_dir, config_path=None)
            statistics_summary = get_statistics_summary(profiling, output_dir=current_output_dir)

            get_data_dictionary(profiling, output_dir=current_output_dir)
            logging.info(f"Dataset {id_data}: Successfully generated Pandas Profiling.")
            prep_data, columns_to_drop = prepare_for_mljar(data=data_df, profiling=profiling,
                                                           csv_data=csv_detective_data)
            logging.info(f"Dataset {id_data}: removed the following columns: {columns_to_drop}")
            logging.info(f"Dataset {id_data}: the following columns are left: {list(prep_data.columns)}")
            if prep_data is not None and len(prep_data.columns) < 3:
                logging.warning(f"Dataset {id_data}: We have less than 3 columns. "
                                f"We will only generate the pandas profiling")
                fill_main_csv(id_=id_data, catalog=catalog, output_dir=output_dir,
                              statistics_summary=statistics_summary, score='',task='EDA')
                continue
            for target_variable in prep_data.columns:
                try:
                    logging.info(f"Dataset {id_data}: Testing AutoML models with target var {target_variable}")
                    # drop nan lines
                    notna_data = prep_data[prep_data[target_variable].notna()]
                    mljar_output_dir = current_output_dir.joinpath(f"automl_{target_variable}")
                    automl = generate_mljar(data=notna_data, target_variable=target_variable,
                                            output_dir=mljar_output_dir, automl_mode=automl_mode)
                    get_mljar_info(output_dir=mljar_output_dir, automl_report=automl)
                    # plot_mljar_table(id)
                    logging.info(f"Dataset {id_data}: Successfully generated AutoML report.")
                    task = automl._get_ml_task()
                    score = generate_score(statistics_summary=statistics_summary, columns_to_drop=columns_to_drop,
                                           automl=automl)
                    logging.info(f"Dataset {id_data}: The score is: {score[0]}")
                    fill_main_csv(id_=id_data, catalog=catalog, statistics_summary=statistics_summary,
                                  output_dir=output_dir, target_variable=target_variable,
                                  task=task, score=score[0], automl=automl)
                    logging.info(f"Dataset {id_data}: Added info to main datasets csv.")
                except Exception:
                    logging.exception(f"Dataset {id_data}: Fatal error while testing var {target_variable}")
                    continue

        except Exception:
            logging.exception(f"Dataset {path}: Fatal error while treating file")


if __name__ == "__main__":
    fire.Fire(main)
