{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning pour prédire la concentration horaire de polluants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce notebook, vous trouvez notre code d'entraînement et validation automatique d'algorithmes de Machine Learning pour prédire la concentration horaire de polluants à partir du jeu de données *Concentration horaire des polluants - Air ambiant - Lig'Air - Orléans Métropole*. <br>\n",
    "On s'intéressera à une tâche de régression et à une tâche de classification sur ce même dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dataset:**https://www.data.gouv.fr/fr/datasets/concentration-horaire-des-polluants-air-ambiant-ligair-orleans-metropole/ <br>\n",
    "**tâche 1:** Régression <br>\n",
    "**target variable 1**:*valeur* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tâche 2:** Classification (Classification multi-classe) <br>\n",
    "**target variable 2**:*nom_pollu* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "#pour l'entraînement automatique:\n",
    "from supervised.automl import AutoML\n",
    "#pour générer un rapport html des résultats:\n",
    "import IPython\n",
    "import markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des données\n",
    "\n",
    "On importe le dataset à partir de son url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.data.gouv.fr/fr/datasets/r/ce203343-6ed9-4fd3-b310-e553ae437f6d'\n",
    "data_conc_pollu = pd.read_csv(url,sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing et feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On élimine les données non exploitables (i.e. telles que *statut_valid* = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_conc_pollu = data_conc_pollu[data_conc_pollu['statut_valid']==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On modifie le type de certaines variables et on crée des nouvelles variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#création des colonnes latitude et longitude\n",
    "data_conc_pollu['lat'] = data_conc_pollu['geo_point_2d'].str.split(',').str[0].astype(float)\n",
    "data_conc_pollu['long'] = data_conc_pollu['geo_point_2d'].str.split(',').str[1].astype(float)\n",
    "\n",
    "#modification du type des variables catégorielles\n",
    "categorical_var = ['nom_com','insee_com','nom_station','code_station','typologie','influence','nom_poll','id_poll_ue']\n",
    "for c in categorical_var:\n",
    "    data_conc_pollu[c] = data_conc_pollu[c].astype('category').cat.codes\n",
    "\n",
    "#modification du type des variables en datetime\n",
    "data_conc_pollu['date_debut'] = pd.to_datetime(data_conc_pollu['date_debut'],format='%Y-%m-%dT%H:%M:%S', utc=True)\n",
    "data_conc_pollu['date_fin'] = pd.to_datetime(data_conc_pollu['date_fin'],format='%Y-%m-%dT%H:%M:%S', utc=True)\n",
    "\n",
    "#création de nouvelles colonnes\n",
    "data_conc_pollu['jour_semaine_debut'] = data_conc_pollu['date_debut'].dt.weekday\n",
    "data_conc_pollu['jour_semaine_fin'] = data_conc_pollu['date_fin'].dt.weekday\n",
    "data_conc_pollu['jour_debut'] = data_conc_pollu['date_debut'].dt.day\n",
    "data_conc_pollu['jour_fin'] = data_conc_pollu['date_fin'].dt.day\n",
    "data_conc_pollu['heure_debut'] = data_conc_pollu['date_debut'].dt.hour\n",
    "data_conc_pollu['heure_fin'] = data_conc_pollu['date_fin'].dt.hour\n",
    "data_conc_pollu['minute_debut'] = data_conc_pollu['date_debut'].dt.minute\n",
    "data_conc_pollu['minute_fin'] = data_conc_pollu['date_fin'].dt.minute\n",
    "\n",
    "#élimination de colonnes\n",
    "data_conc_pollu = data_conc_pollu.drop(columns=['nom_dept','metrique','unite','code_epci','statut_valid','geo_shape','geo_point_2d','date_debut','date_fin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Régression\n",
    "\n",
    "### 3.1. Train/Test splitting\n",
    "\n",
    "On définit la variable réponse et les variables explicatives.\n",
    "On partage le dataset en dataset d'entraînement et de test. Attention, AutoML nécessite d'un partage 75%/25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_reg = data_conc_pollu['valeur'].values\n",
    "X_reg = data_conc_pollu.drop(columns=['valeur'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. AutoML\n",
    "\n",
    "On entraîne, test et valide automatiquement plusieurs algorithmes de ML sur le dataset avec AutoML (plus d'infos ici: https://supervised.mljar.com/) et on génère un html des métriques et des paramètres du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML(total_time_limit=5*60,mode='Explain',random_state=42)\n",
    "#fit model\n",
    "automl.fit(X_train_reg,y_train_reg)\n",
    "#predictions\n",
    "predictions = automl.predict(X_test_reg)\n",
    "#generate report\n",
    "automl.report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classification\n",
    "\n",
    "### 4.1. Train/Test splitting\n",
    "On définit la variable réponse et les variables explicatives. On partage le dataset en dataset d'entraînement et de test. Attention, AutoML nécessite d'un partage 75%/25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = data_conc_pollu['nom_poll'].values\n",
    "X_class = data_conc_pollu.drop(columns=['nom_poll','id_poll_ue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML(total_time_limit=5*60,mode='Explain',random_state=42)\n",
    "#fit model\n",
    "automl.fit(X_train,y_train)\n",
    "#predictions\n",
    "predictions = automl.predict(X_test)\n",
    "#generate html report\n",
    "automl.report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
